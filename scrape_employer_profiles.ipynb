{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFQIIfKPjve4"
   },
   "source": [
    "# Scraping all Kununu Websites for German Firms\n",
    "\n",
    "Requires scraping three different pages:\n",
    "- Main Page: https://www.kununu.com/de/[company name] \n",
    "- Total Views: https://www.kununu.com/middlewares/profiles/+[company uuid]+/statistics \n",
    "- Applicant Reviews: https://www.kununu.com/de/[company name]/bewerbung\n",
    "- Employee Reviews: https://www.kununu.com/de/[company name]/kommentare\n",
    "\n",
    "**Important Note**: This code works as of December 10th, 2024. Kununu may change their website structure, which would require updating the code.\n",
    "In particular, it is likely that the CSS_CLASSES dictionary in config.py will need to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests beautifulsoup4 pandas numpy python-dotenv\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from config import *\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "# load_dotenv() # make sure to have a .env file that defines the variable 'SCRAPINGBEE_API_KEY' if using scrapingbee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY = 100 # Number of concurrent requests to make when scraping\n",
    "\n",
    "# importing all Kununu links\n",
    "pwd = os.getcwd()\n",
    "with open(pwd+\"/data/all_kununu_company_profile_links.txt\", \"r\") as file:\n",
    "    FileContent = file.read()\n",
    "all_kununu_links = FileContent.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_yIg22RMZpWr"
   },
   "outputs": [],
   "source": [
    "def get_stats_page(uuid: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"Scrape stats page, return fully flattened dictionary with unstacked recommendation_rate.\"\"\"\n",
    "    response = requests.get(f\"https://www.kununu.com/middlewares/profiles/{uuid}\")\n",
    "    data_dict = replace_null_with_none(response.text)\n",
    "    data_dict = flatten(convert_keys_to_snake_case(data_dict))\n",
    "\n",
    "    # Manually un-nest recommendation_rate\n",
    "    recommendation_rate = data_dict.get(\"reviews\", {}).get(\"recommendation_rate\", {})\n",
    "    recommendation_rate_unstacked = {\n",
    "        \"recommendation_rate_percentage\": recommendation_rate.get(\"percentage\"),\n",
    "        \"recommendation_rate_total_reviews\": recommendation_rate.get(\"total_reviews\"),\n",
    "        \"recommendation_rate_recommended_total_reviews\": recommendation_rate.get(\"recommended_total_reviews\"),\n",
    "        \"recommendation_rate_not_recommended_total_reviews\": recommendation_rate.get(\"not_recommended_total_reviews\"),\n",
    "    }\n",
    "    data_dict.update(recommendation_rate_unstacked)\n",
    "    data_dict.pop(\"reviews_recommendation_rate\", None)\n",
    "\n",
    "    data_dict = {key: data_dict.get(key) for key in required_keys}\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def main_page_scrape(url: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a URL and returns a dictionary with all the information scraped from the URL to ratings_list.\n",
    "    Information collected:\n",
    "    - Firm name\n",
    "    - Firm uuid\n",
    "    - Number of views\n",
    "    - Overall rating\n",
    "    - Percentage of people who would recommend the firm\n",
    "    - Total number of reviews\n",
    "    - Number of salaries posted\n",
    "    - Number of corporate culture reviews\n",
    "    - Ratings for each category\n",
    "    - Benefits\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    soup = soup_from_url(url)\n",
    "\n",
    "    result_dict[\"url\"] = url\n",
    "    result_dict[\"uuid\"] = str(soup).split('\"uuid\":\"')[1].split('\"')[0] if '\"uuid\":\"' in str(soup) else None\n",
    "\n",
    "    num_revs_soup = re.findall(r'\\(.*?\\)', str(soup.find(class_=CSS_CLASSES[\"tabs\"]).text)) if soup.find(class_=CSS_CLASSES[\"tabs\"]) else None\n",
    "    if num_revs_soup:\n",
    "        num_revs_ls = [int(x.replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\")) for x in num_revs_soup]\n",
    "        result_dict[\"salaries_posted_num\"] = num_revs_ls[1] if len(num_revs_ls) > 1 else np.nan\n",
    "        result_dict[\"corporate_culture_review_num\"] = num_revs_ls[2] if len(num_revs_ls) > 2 else np.nan\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Failed to retrieve number of reviews, salaries posted, or corporate culture reviews.\")\n",
    "        result_dict[\"total_reviews_num\"], result_dict[\"salaries_posted_num\"], result_dict[\"corporate_culture_review_num\"] = np.nan, np.nan, np.nan\n",
    "    \n",
    "    if verbose:\n",
    "        try:    \n",
    "            # Retrieve Satisfied Salary Percentage\n",
    "            satisfied_salary_section = soup.find(class_=CSS_CLASSES[\"satisfied_salary\"])\n",
    "            \n",
    "            # Extract the percentage value, if available\n",
    "            if satisfied_salary_section:\n",
    "                result_dict[\"satisfied_salary_pct\"] = int(\n",
    "                    satisfied_salary_section.text.split()[0].strip().replace(\"%\", \"\")\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Satisfied salary section not found in the HTML. Ensure that the page structure has not changed.\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Failed to retrieve satisfied salary percentage: {e}\")\n",
    "            result_dict[\"satisfied_salary_pct\"] = np.nan\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        try:\n",
    "            benefits_section = soup.find(class_=CSS_CLASSES[\"benefits\"])\n",
    "            \n",
    "            if not benefits_section:\n",
    "                raise ValueError(\"Benefits section not found in the HTML. Ensure that the page structure has not changed.\")\n",
    "            \n",
    "            benefit_items = benefits_section.find_all(\"li\", class_=CSS_CLASSES[\"benefit_items\"])\n",
    "            \n",
    "            if not benefit_items:\n",
    "                raise ValueError(\"No benefit items found in the benefits section. Verify if benefits are listed properly or the HTML class has been updated.\")\n",
    "            \n",
    "            for benefit_item in benefit_items:\n",
    "                try:\n",
    "                    benefit_name = benefit_item.find(\"span\", class_=CSS_CLASSES[\"benefit_title\"]).text.strip()\n",
    "                    benefit_percentage = benefit_item.find(\"div\", class_=CSS_CLASSES[\"benefit_percentage\"]).find(\"span\", class_=\"legend-regular\").text.strip().replace(\"%\", \"\")\n",
    "                    result_dict[benefit_name] = int(benefit_percentage)\n",
    "                except AttributeError as inner_e:\n",
    "                    if verbose:\n",
    "                        print(f\"Failed to extract name or percentage for a benefit item: {inner_e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Failed to retrieve benefits data: {e}\")\n",
    "    else:\n",
    "        benefits_section = soup.find(class_=CSS_CLASSES[\"benefits\"])\n",
    "        if benefits_section:\n",
    "            benefit_items = benefits_section.find_all(\"li\", class_=CSS_CLASSES[\"benefit_items\"])\n",
    "            for benefit_item in benefit_items:\n",
    "                try:\n",
    "                    benefit_name = benefit_item.find(\"span\", class_=CSS_CLASSES[\"benefit_title\"]).text.strip()\n",
    "                    benefit_percentage = benefit_item.find(\"div\", class_=CSS_CLASSES[\"benefit_percentage\"]).find(\"span\", class_=\"legend-regular\").text.strip().replace(\"%\", \"\")\n",
    "                    result_dict[benefit_name] = int(benefit_percentage)\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "\n",
    "    ratings_raw = [x.parent.text for x in soup.find_all(class_=re.compile(CSS_CLASSES[\"factor_score\"]))]\n",
    "    categories = [rating[3:] for rating in ratings_raw]\n",
    "    ratings = [float(str(rating[:3].replace(\",\", \".\"))) for rating in ratings_raw]\n",
    "    result_dict.update(dict(zip(categories, ratings)))\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def get_applicant_info(url: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns the review scores and number of reviews by applicants to the company,\n",
    "    separated by the following categories: \"hired\", \"rejected\", \"offerDeclined\", \"deferred\".\n",
    "    \"\"\"\n",
    "    application_outcomes = [\"hired\", \"rejected\", \"offerDeclined\", \"deferred\"]\n",
    "    reviews_by_applicants = {}\n",
    "\n",
    "    for outcome in [\"all_applicants\"] + application_outcomes:\n",
    "        soup = soup_from_url(f\"{url}/bewerbung{'?result=' + outcome if outcome != 'all_applicants' else ''}\")\n",
    "        try:\n",
    "            reviews_by_applicants[f\"{outcome}_review_num\"] = int(soup.find(class_=CSS_CLASSES[\"total_reviews\"]).text.split(\" \")[0])\n",
    "            reviews_by_applicants[f\"{outcome}_review_score\"] = float(soup.find(class_=CSS_CLASSES[\"aggregation\"]).text[:3].replace(\",\", \".\"))\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Failed to retrieve review number or score for outcome '{outcome}': {e}\")\n",
    "            reviews_by_applicants[f\"{outcome}_review_num\"] = np.nan\n",
    "            reviews_by_applicants[f\"{outcome}_review_score\"] = np.nan\n",
    "\n",
    "    return reviews_by_applicants\n",
    "\n",
    "def get_employee_info(url: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns the review scores, number of reviews by employees to the company,\n",
    "    and percent of employees that would recommend the company.\n",
    "    \"\"\"\n",
    "    reviews_by_employees = {\n",
    "        \"sehr_gut_reviews\": np.nan,\n",
    "        \"gut_reviews\": np.nan,\n",
    "        \"befriedigend_reviews\": np.nan,\n",
    "        \"genuegend_reviews\": np.nan,\n",
    "    }\n",
    "    soup = soup_from_url(f\"{url}/kommentare\")\n",
    "\n",
    "    try:\n",
    "        reviews_by_employees[\"employees_review_num\"] = int(soup.find(class_=CSS_CLASSES[\"total_reviews\"]).text.split(\" \")[0].replace(\".\", \"\"))\n",
    "        reviews_by_employees[\"employee_review_score\"] = float(soup.find(class_=CSS_CLASSES[\"employee_score\"]).text[:3].replace(\",\", \".\"))\n",
    "        reviews_by_employees[\"employee_rec_score\"] = int(soup.find(class_=CSS_CLASSES[\"employee_recommendation\"]).text.split(\"%\")[0])\n",
    "\n",
    "        # Adding employee review scores for each category\n",
    "        category_buttons = soup.find_all(\"button\", {\"aria-label\": \"Reviews score detail\"})\n",
    "        for button in category_buttons:\n",
    "            category_name_german = button.find(\"span\", class_=\"index__category__fvg57\").text.strip()\n",
    "            review_count_text = button.find(\"span\", class_=\"index__totalReviews__6pCSR\").text.split(\" \")[0]\n",
    "            review_count = int(review_count_text.replace(\".\", \"\").replace(\",\", \"\"))\n",
    "\n",
    "            # Match against German category names\n",
    "            if category_name_german == \"Sehr gut\":\n",
    "                reviews_by_employees[\"sehr_gut_reviews\"] = review_count\n",
    "            elif category_name_german == \"Gut\":\n",
    "                reviews_by_employees[\"gut_reviews\"] = review_count\n",
    "            elif category_name_german == \"Befriedigend\":\n",
    "                reviews_by_employees[\"befriedigend_reviews\"] = review_count\n",
    "            elif category_name_german == \"Genügend\":\n",
    "                reviews_by_employees[\"genuegend_reviews\"] = review_count\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Failed to retrieve employee review information: {e}\")\n",
    "        reviews_by_employees[\"employees_review_num\"] = np.nan\n",
    "        reviews_by_employees[\"employee_review_score\"] = np.nan\n",
    "        reviews_by_employees[\"employee_rec_score\"] = np.nan\n",
    "\n",
    "    return reviews_by_employees\n",
    "\n",
    "def get_all_info(url: str, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns all the information scraped from the URL to a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    result_dict = main_page_scrape(url, verbose=verbose)\n",
    "    result_dict.update(get_applicant_info(url, verbose=verbose))\n",
    "    result_dict.update(get_employee_info(url, verbose=verbose))\n",
    "    result_dict.update(get_stats_page(result_dict[\"uuid\"]))\n",
    "    df = pd.DataFrame([result_dict])\n",
    "    df.columns = [x.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"\").replace(\"ä\", \"a\").lower() for x in df.columns]\n",
    "    df = df.rename(columns=column_name_mapping)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping all data in parallel and saving to csv\n",
    "window_size = 5\n",
    "for i in range(0, len(all_kununu_links)//window_size+1):\n",
    "    concurrency = CONCURRENCY\n",
    "    pool = ThreadPool(concurrency)\n",
    "    df = pool.map(get_all_info, all_kununu_links[i*window_size:min((i+1)*window_size, len(all_kununu_links))])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    df.to_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\", index=False)\n",
    "    print(f\"Saved results to data/kununu_data_{i+1}.csv\")\n",
    "\n",
    "# Consolidate all results\n",
    "df = pd.concat([pd.read_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\") for i in range(0, len(all_kununu_links)//window_size+1)], ignore_index=True)\n",
    "df.to_csv(f\"{pwd}/data/kununu_data.csv\", index=False)\n",
    "print(\"Saved results to data/kununu_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vlh9acpqLAmT"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
